---
#
# By default, content added below the "---" mark will appear in the home page
# between the top bar and the list of recent posts.
# To change the home page layout, edit the _layouts/home.html file.
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
#
layout: default
---

<style>
    p, li {font-weight: 300;}
</style>
<!-- (comment) the image below can be found in img folder of this very project-->

<!-- ![me](/resources/people/me_3.jpg){: style="float: right; margin: 40px 30px; width: 180px;"} -->
<figure style="float: right; margin: 40px 30px;">
  <img src="/resources/people/me.jpg" alt="Me" style="width: 180px;">
  <!-- <figcaption style="display: block; text-align: center;"><span class="note">(Twister and me)</span></figcaption> -->
</figure>


Hi there!

I'm a final-year PhD candidate at the Department of [CSAI](https://www.tilburguniversity.edu/about/schools/tshd/departments/dca) at [Tilburg University](https://www.tilburguniversity.edu/), the Netherlands. I'm part of the consortium project [InDeep: Interpreting Deep Learning Models for Text and Sound](https://projects.illc.uva.nl/indeep/), and happy to be supervised by [Afra Alishahi](http://afra.alishahi.name/), [Willem Zuidema](https://staff.fnwi.uva.nl/w.zuidema/), and [Grzegorz Chrupa≈Ça](https://grzegorz.chrupala.me/).



<!-- My research focuses on __analyzing and interpreting deep neural models of language__ (written or spoken) by treating them as mathematical functions. I try to develop analysis methods that can faithfully elucidate the interplay and flow of information within neural networks. -->
My research focuses on __interpreting deep neural language models__ (both written and spoken). I try to develop analysis methods to trace information flow and contextual interactions within these models, aiming to understand their inner workings and make that understanding useful for something such as improving model efficiency, controllability, and safety.

## Background
<!-- ## <span style="color:#424242">Background </span> -->
I was a visiting researcher (Jan-Mar 2024) at [ILCC, School of Informatics, University of Edinburgh](https://web.inf.ed.ac.uk/ilcc), worked with [Ivan Titov](http://ivan-titov.org/). 

I completed my Master‚Äôs (2019-2021) in Artificial Intelligence at [Iran University of Science and Technology](http://www.iust.ac.ir/en), where my research revolved around the interpretation of pre-trained language models and the utilization of interpretability techniques to accelerate their inference time, under the supervision of [Mohammad Taher Pilehvar](https://pilehvar.github.io/). 

Before that, I got my Bachelor‚Äôs (2014-2019) in Computer Engineering from [Ferdowsi University of Mashhad](https://en.um.ac.ir/). During that, I was working under the supervision of [Ahad Harati](http://a.harati.profcms.um.ac.ir/) as a team member of the Nexus RoboCup Simulation Team. 


## Public Activities 
<!-- ## <span style="color:#424242">Services </span> -->
* Workshop Organizer
  * [BlackboxNLP](https://blackboxnlp.github.io/) (co-located with EMNLP 2023, 2024, 2025)
* Tutorial Instructor
  * <span class="emoji">üí•</span>__Upcoming__ Tutorial on [_"Interpretability Techniques for Speech Models"_](https://interpretingdl.github.io/speech-interpretability-tutorial/) at [Interspeech 2025](https://www.interspeech2025.org/tutorials) conference
  * Tutorial on [_"Transformer-specific Interpretability"_](https://projects.illc.uva.nl/indeep/tutorial/) at [EACL 2024](https://2024.eacl.org/program/tutorials/) conference
* Area Chair / Meta-reviewer
  * _"Interpretability and Analysis of Models for NLP"_ track for ACL Rolling Review 2025 
  * _"Speech Recognition, Text-to-Speech and Spoken Language Understanding"_ track for ACL Rolling Review 2025
* Session Chair
  * _Speech Processing_ oral session at ACL 2025, Vienna
* Program Committee / Reviewer
  * Conferences: EMNLP (2022, 2023), ACL 2023, EACL 2023, ACL Rolling Review (2022, 2023)
  * Workshops: Actionable Interpretability (ICML 2025) 
* Others
  * I organized [InDeep Journal Club](https://projects.illc.uva.nl/indeep/events/journal-club/) (2022-2024)


## Highlighted news
<!-- ## <span style="color:#424242">News and Activities </span> -->
* __May 2025__: Two accepted papers to Interspeech 2025: [language-specific pretraining (Wav2Vec2-NL)](https://arxiv.org/abs/2506.00981), and [on the reliability of feature attribution for speech](https://arxiv.org/abs/2505.16406)
* __Dec 2024__: <span class="emoji">üì∫</span> A series of short videos on Transformer Interpretability available on [YouTube](https://youtu.be/JPOBPY-ndfk?si=vQ2JdME8oo_6iFCU)!
* __Oct 2024__: Check out our new preprint: [Disentangling Textual and Acoustic Features of Neural Speech Representations](https://arxiv.org/abs/2410.03037)
<!-- * __Sep 2024__: A new paper accepted to BlackboxNLP 2024: [How Language Models Prioritize Contextual Grammatical Cues?](https://arxiv.org/abs/2410.03447) -->
<!-- * __Apr 2024__: I'll run for Spierfonds to raise money for research on muscular diseases. Your donation [here](https://www.spieractie.nl/fundraisers/hosein-mohebbi) would greatly please me! -->
* __Mar 2024__: Materials (slides, notebooks, etc.) for [EACL 2024](https://2024.eacl.org/program/tutorials/) tutorial on "Transformer-specific Interpretability" are available [here](https://github.com/interpretingdl/eacl2024_transformer_interpretability_tutorial).
<!--* __Mar 2024__: [DecoderLens](https://arxiv.org/abs/2310.03686) has been accepted to findings of [NAACL 2024](https://2024.naacl.org/). -->
<!-- * __Jan 2024__: Started a research visit at ILCC, University of Edinburgh. -->
* __Dec 2023__: <span class="emoji">üèÖ</span> Got an Outstanding Paper Award for [Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers](https://aclanthology.org/2023.emnlp-main.513/) at EMNLP 2023!
<!-- * __Oct 2023__: New [paper](https://arxiv.org/abs/2310.09925) on model interpretability for spoken language accepted to [EMNLP'23](https://2023.emnlp.org/) main! -->
<!-- * __Jun 2023__: Invited talk on "Context Mixing in Transformers" at [GroNLP](https://www.rug.nl/research/clcg/research/cl/), University of Groningen. -->
<!-- * __May 2023__: Gave a guest lecture on Transformers to an undergraduate CL course at Tilburg University. -->
* __Mar 2023__: Blog [Post](https://hmohebbi.github.io/blog/value-zeroing): A few thoughts on why Value Zeroing. 
<!-- * __Jan 2023__: [Value Zeroing](https://arxiv.org/abs/2301.12971) is out, a new interpretability method customized for Transformers (accepted to [EACL'23](https://2023.eacl.org/) main conference).  -->
<!-- * __Jan 2023__: Presented a poster at [ALiAS'23](https://staff.fnwi.uva.nl/w.zuidema/alias-2023/).  -->
<!-- * __Dec 2022__: [BlackboxNLP](https://blackboxnlp.github.io/) will be back in 2023 at EMNLP! Happy to be serving as a co-organizer.  -->
<!-- * __Sep 2022__: Gave a guest lecture on "Interpretability of Transformers" to a graduate Advanced Deep Learning course at Tilburg University. \[[slides](https://drive.google.com/file/d/1Z39YSfzpzzkqAiMxVdW1nOkudfYVgj_y/view?usp=sharing)\] -->
<!-- * __May 2022__: Gave a short talk at [InDeep](https://projects.illc.uva.nl/indeep/) workshop at the University of Amsterdam. -->
<!-- * __Feb 2022__: [AdapLeR](https://aclanthology.org/2022.acl-long.1/) is out, up to 22x infrence speedup while retaining performance ([ACL'22](https://www.2022.aclweb.org/) main).
<!-- * __Nov 2021__: Moved to the Netherlands to join the consortium project: [InDeep](https://interpretingdl.github.io/). -->
<!-- * __Sep 2021__: <span class="emoji">üéì</span> Successfully defended my Master's thesis titled "Interpretability and Transferability of Linguistic Knowledge in Pre-trained Language Models". -->
<!-- * __Sep 2021__: Two papers accepted to [EMNLP'21](https://2021.emnlp.org/) (main conference and BlackboxNLP). -->
<!-- * __Jun 2021__: Invited talk at Cambridge/Cardiff Workshop in Natural Language Processing. -->
<!-- * __May 2021__: Gave a joint guest lecture, with [Ali](https://www.amodarressi.com/), on Interpretability to a graduate [NLP course](https://teias-courses.github.io/nlp99/) at Khatam University. \[[slides](https://drive.google.com/file/d/1cAzlIlbuVAFZXz3gaFGBTRZwjq-_V2lb/view?usp=sharing)\] -->
<!-- * __Apr 2021__: Our pre-print intepretability work is ready! [Exploring the Role of BERT Token Representations to Explain Sentence Probing Results](https://arxiv.org/abs/2104.01477). -->
