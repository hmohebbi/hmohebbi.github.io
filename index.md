---
#
# By default, content added below the "---" mark will appear in the home page
# between the top bar and the list of recent posts.
# To change the home page layout, edit the _layouts/home.html file.
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
#
layout: default
---

<style>
    p, li {font-weight: 300;}
</style>
<!-- (comment) the image below can be found in img folder of this very project-->

<!-- ![me](/resources/people/me_3.jpg){: style="float: right; margin: 40px 30px; width: 180px;"} -->
<figure style="float: right; margin: 40px 30px;">
  <img src="/resources/people/me.jpg" alt="Me" style="width: 180px;">
  <!-- <figcaption style="display: block; text-align: center;"><span class="note">(Twister and me)</span></figcaption> -->
</figure>

Hi there!

I'm a second-year __Ph.D.__ candidate at the Department of [CSAI](https://www.tilburguniversity.edu/about/schools/tshd/departments/dca) at [Tilburg University](https://www.tilburguniversity.edu/). I'm part of the NWO-funded project [InDeep: Interpreting Deep Learning Models for Text and Sound](https://projects.illc.uva.nl/indeep/), and happy to be supervised by [Afra Alishahi](http://afra.alishahi.name/), [Willem Zuidema](https://staff.fnwi.uva.nl/w.zuidema/), and [Grzegorz ChrupaÅ‚a](https://grzegorz.chrupala.me/).

My research focuses on __analyzing and interpreting deep neural models of language__ (both written and spoken) by treating them as mathematical functions, rather than mere black boxes that take inputs and give outputs. I try to develop analysis methods that can faithfully elucidate the interplay and flow of information within neural networks.

## <span style="color:#424242">Background </span>
Previously, I completed my Masterâ€™s degree in Artificial Intelligence at [Iran University of Science and Technology](http://www.iust.ac.ir/en), where my research revolved around the interpretation of pre-trained language models and the utilization of interpretability techniques to accelerate their inference time, under the supervision of [Mohammad Taher Pilehvar](https://pilehvar.github.io/).

Before that, I got my Bachelorâ€™s in Computer Engineering from [Ferdowsi University of Mashhad](https://en.um.ac.ir/). During that, I was working under the supervision of [Ahad Harati](http://a.harati.profcms.um.ac.ir/) as a team member of the Nexus RoboCup Simulation Team.



## <span style="color:#424242">Services </span>
* I'm a co-organizer of: [InDeep](https://projects.illc.uva.nl/indeep/) Journal Club, [BlackboxNLP 2023](https://blackboxnlp.github.io/) Workshop
* I reviewed for the following conferences: EMNLP'23, ACL'23, EACL'23, ACL Rolling Review 2022

## <span style="color:#424242">News </span>
* __Jun 2023__: Invited talk on "context mixing in Transformers" at [GroNLP](https://www.rug.nl/research/clcg/research/cl/), University of Groningen.
* __May 2023__: Gave a guest lecture on Transformers to an undergraduate CL course at Tilburg University.
* __Mar 2023__: New blog [post](https://hmohebbi.github.io/blog/value-zeroing): A few thoughts on why Value Zeroing.
* __Jan 2023__: ðŸ¥³ [Value Zeroing](https://arxiv.org/abs/2301.12971) is out, a new interpretability method customized for Transformers (accepted to [EACL'23](https://2023.eacl.org/) main conference).
* __Jan 2023__: Presented a poster at [ALiAS'23](https://staff.fnwi.uva.nl/w.zuidema/alias-2023/).
* __Dec 2022__: [BlackboxNLP](https://blackboxnlp.github.io/) will be back in 2023 at EMNLP! Happy to be serving as a co-organizer.
* __Sep 2022__: Gave a guest lecture on "Interpretability of Transformers" to a graduate Advanced Deep Learning course at Tilburg University. \[[slides](https://drive.google.com/file/d/1Z39YSfzpzzkqAiMxVdW1nOkudfYVgj_y/view?usp=sharing)\]
* __May 2022__: Gave a short talk at [InDeep](https://projects.illc.uva.nl/indeep/) workshop at the University of Amsterdam.
* __Feb 2022__: ðŸ¥³ [AdapLeR](https://aclanthology.org/2022.acl-long.1/) is out, up to 22x infrence speedup while retaining performance ([ACL'22](https://www.2022.aclweb.org/) main).
* __Nov 2021__: Moved to the Netherlands to join the consortium project: [InDeep](https://interpretingdl.github.io/).
* __Sep 2021__: ðŸŽ“ Successfully defended my Master's thesis titled "Interpretability and Transferability of Linguistic Knowledge in Pre-trained Language Models".
* __Sep 2021__: ðŸ¥³ Two papers accepted to [EMNLP'21](https://2021.emnlp.org/) (main conference and BlackboxNLP).
* __Jun 2021__: Invited talk at Cambridge/Cardiff Workshop in Natural Language Processing.
* __May 2021__: Gave a joint guest lecture, with [Ali](https://www.amodarressi.com/), on Interpretability to a graduate [NLP course](https://teias-courses.github.io/nlp99/) at Khatam University. \[[slides](https://drive.google.com/file/d/1cAzlIlbuVAFZXz3gaFGBTRZwjq-_V2lb/view?usp=sharing)\]
* __Apr 2021__: Our pre-print intepretability work is ready! [Exploring the Role of BERT Token Representations to Explain Sentence Probing Results](https://arxiv.org/abs/2104.01477).
