---
layout: default
title: Publications
permalink: /publications/
---

## Publications

<ui>
<b>Disentangling Textual and Acoustic Features of Neural Speech Representations</b>
<br>Hosein Mohebbi, Grzegorz Chrupa≈Ça, Willem Zuidema, Afra Alishahi, Ivan Titov<br>
<i>preprint</i> 2024<br>
<a href="https://arxiv.org/abs/2410.03037" target="_blank">paper</a> | <a href="https://github.com/hmohebbi/disentangling_representations" target="_blank">code</a> | <a href="https://youtu.be/QkDFCOf2tXM" target="_blank">video</a> 
<br><br>
</ui>

<ui>
<b>What Do Self-supervised Speech Models Know about Dutch? Analyzing Advantages of Language-specific Pretraining</b>
<br>Marianne de Heer Kloots, Hosein Mohebbi, Charlotte Pouw, Gaofei Shen, Willem Zuidema, Martijn Bentum<br>
<i>In Proceedings of Interspeech</i> 2025<br>
<a href="https://arxiv.org/abs/2506.00981" target="_blank">paper</a> | <a href="" target="_blank">model checkpoints</a>
<br><br>
</ui>

<ui>
<b>On the Reliability of Feature Attribution Methods for Speech Classification</b>
<br>Gaofei Shen, Hosein Mohebbi, Arianna Bisazza, Afra Alishahi, Grzegorz Chrupa≈Ça<br>
<i>In Proceedings of Interspeech</i> 2025<br>
<a href="https://arxiv.org/abs/2505.16406" target="_blank">paper</a>
<br><br>
</ui>

<ui>
<b>How Language Models Prioritize Contextual Grammatical Cues?</b>
<br>Hamidreza Amirzadeh, Afra Alishahi, Hosein Mohebbi<br>
<i>In Proceedings of BlackboxNLP</i> 2024<br>
<a href="https://arxiv.org/abs/2410.03447" target="_blank">paper</a>
<br><br>
</ui>

<ui>
<b>DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers</b>
<br>Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, Jaap Jumelet<br>
<i>In Findings of NAACL</i> 2024<br>
<a href="https://aclanthology.org/2024.findings-naacl.296/" target="_blank">paper</a>
<br><br>
</ui>

<ui>
<b>Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers</b>
<br>Hosein Mohebbi, Grzegorz Chrupa≈Ça, Willem Zuidema, Afra Alishahi<br>
<span class="emoji">üèÖ</span> <b>Outstanding Paper Award</b><br>
<i>In Proceedings of EMNLP</i> 2023<br>
<a href="https://aclanthology.org/2023.emnlp-main.513/" target="_blank">paper</a> | <a href="https://huggingface.co/datasets/hosein-m/french_homophone_asr" target="_blank">data</a> | <a href="https://github.com/hmohebbi/ContextMixingASR" target="_blank">code</a>
<br><br>
</ui>

<ui>
<b>Quantifying Context Mixing in Transformers</b>
<br>Hosein Mohebbi, Willem Zuidema, Grzegorz Chrupa≈Ça, Afra Alishahi<br>
<i>In Proceedings of EACL</i> 2023<br>
<a href="https://aclanthology.org/2023.eacl-main.245/" target="_blank">paper</a> | <a href="https://github.com/hmohebbi/ValueZeroing" target="_blank">code</a> | <a href="https://hmohebbi.github.io/blog/value-zeroing" target="_blank">blog</a> | <a href="https://huggingface.co/spaces/amsterdamNLP/value-zeroing" target="_blank">demo</a>
<br><br>
</ui>

<ui>
<b>AdapLeR: Speeding up Inference by Adaptive Length Reduction</b>
<br>Ali Modarressi, Hosein Mohebbi, Mohammad Taher Pilehvar<br>
<i>In Proceedings of ACL</i> 2022<br>
<a href="https://aclanthology.org/2022.acl-long.1/" target="_blank">paper</a> | <a href="https://github.com/amodaresi/AdapLeR" target="_blank">code</a> | <a href="http://www.amodarressi.com/AdapLeR/" target="_blank">blog</a> 
<br><br>
</ui>

<ui>
<b>Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids' Representations</b>
<br>Mohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Hosein Mohebbi, Mohammad Taher Pilehvar<br>
<i>In Proceedings of BlackboxNLP</i> 2021<br>
<a href="https://aclanthology.org/2021.blackboxnlp-1.29/" target="_blank">paper</a>
<br><br>
</ui>

<ui>
<b>Exploring the Role of BERT Token Representations to Explain Sentence Probing Results</b>
<br>Hosein Mohebbi, Ali Modarressi, Mohammad Taher Pilehvar<br>
<i>In Proceedings of EMNLP</i> 2021<br>
<a href="https://aclanthology.org/2021.emnlp-main.61/" target="_blank">paper</a> | <a href="https://github.com/hmohebbi/explain-probing-results" target="_blank">code</a> | <a href="https://hmohebbi.github.io//blog/explain-probing-results" target="_blank">blog</a> 
<br><br>
</ui>
