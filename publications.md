---
layout: default
title: Publications
permalink: /publications/
---

## Publications

<!-- H. Mohebbi, G. Chrupała, W. Zuidema, A. Alishahi. Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers. EMNLP 2023. [[Paper](https://arxiv.org/abs/2310.09925)] [[Code](https://github.com/hmohebbi/ContextMixingASR)]

H. Mohebbi, W. Zuidema, G. Chrupała, A. Alishahi. Quantifying Context Mixing in Transformers. EACL 2023. [[Paper](https://aclanthology.org/2023.eacl-main.245/)] [[Code](https://github.com/hmohebbi/ValueZeroing)] [[Blog](https://hmohebbi.github.io/blog/value-zeroing)] [[Demo](https://huggingface.co/spaces/amsterdamNLP/value-zeroing)]

A. Modarressi, H. Mohebbi, M. T. Pilehvar. AdapLeR: Speeding up Inference by Adaptive Length Reduction. ACL 2022. [[Paper](https://aclanthology.org/2022.acl-long.1/)] [[Code](https://github.com/amodaresi/AdapLeR)] [[Blog](http://www.amodarressi.com/AdapLeR/)]

M. Fayyaz, E. Aghazadeh, A. Modarressi, H. Mohebbi, M. T. Pilehvar. Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids' Representations. BlackboxNLP 2021. [[Paper](https://aclanthology.org/2021.blackboxnlp-1.29/)]

H. Mohebbi, A. Modarressi, M. T. Pilehvar. Exploring the Role of BERT Token Representations to Explain Sentence Probing Results. EMNLP 2021. [[Paper](https://aclanthology.org/2021.emnlp-main.61/)] [[Code](https://github.com/hmohebbi/explain-probing-results)] [[Blog](https://hmohebbi.github.io//blog/explain-probing-results)] -->

<ui>
<b>Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers</b>
<br>Hosein Mohebbi, Grzegorz Chrupała, Willem Zuidema, Afra Alishahi<br>
<i>In Proceedings of EMNLP</i> 2023<br>
<a href="https://arxiv.org/abs/2310.09925" target="_blank">paper</a> | <a href="https://github.com/hmohebbi/ContextMixingASR" target="_blank">code</a>
<br><br>
</ui>

<ui>
<b>DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers</b>
<br>Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, Jaap Jumelet<br>
<i>Preprint</i> 2023<br>
<a href="https://arxiv.org/abs/2310.03686" target="_blank">paper</a>
<br><br>
</ui>

<ui>
<b>Quantifying Context Mixing in Transformers</b>
<br>Hosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, Afra Alishahi<br>
<i>In Proceedings of EACL</i> 2023<br>
<a href="https://aclanthology.org/2023.eacl-main.245/" target="_blank">paper</a> | <a href="https://github.com/hmohebbi/ValueZeroing" target="_blank">code</a> | <a href="https://hmohebbi.github.io/blog/value-zeroing" target="_blank">blog</a> | <a href="https://huggingface.co/spaces/amsterdamNLP/value-zeroing" target="_blank">demo</a>
<br><br>
</ui>

<ui>
<b>AdapLeR: Speeding up Inference by Adaptive Length Reduction</b>
<br>Ali Modarressi, Hosein Mohebbi, Mohammad Taher Pilehvar<br>
<i>In Proceedings of ACL</i> 2022<br>
<a href="https://aclanthology.org/2022.acl-long.1/" target="_blank">paper</a> | <a href="https://github.com/amodaresi/AdapLeR" target="_blank">code</a> | <a href="http://www.amodarressi.com/AdapLeR/" target="_blank">blog</a> 
<br><br>
</ui>

<ui>
<b>Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids' Representations</b>
<br>Mohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Hosein Mohebbi, Mohammad Taher Pilehvar<br>
<i>In Proceedings of BlackboxNLP 2021</i> 2023<br>
<a href="https://aclanthology.org/2021.blackboxnlp-1.29/" target="_blank">paper</a>
<br><br>
</ui>

<ui>
<b>Exploring the Role of BERT Token Representations to Explain Sentence Probing Results</b>
<br>Hosein Mohebbi, Ali Modarressi, Mohammad Taher Pilehvar<br>
<i>In Proceedings of EMNLP</i> 2021<br>
<a href="https://aclanthology.org/2021.emnlp-main.61/" target="_blank">paper</a> | <a href="https://github.com/hmohebbi/explain-probing-results" target="_blank">code</a> | <a href="https://hmohebbi.github.io//blog/explain-probing-results" target="_blank">blog</a> 
<br><br>
</ui>